# Chaos Engineering класстера 

## Предисловие
---
Все тесты проводились на нагруженной системе с использованием тестового сценария, разработанного в предыдущей задаче. Нагрузка на систему составляла 400 запросов в секунду (rps).

## Задачи
---
### 1. Отключение узла

**План:**
- Планово остановить один из узлов кластера, чтобы проверить процедуру переключения ролей (failover).
- Анализировать время, необходимое для восстановления и как система выбирает новый Master узел (и есть ли вообще там стратегия выбора?).

**Описание эксперимента:**
1. Определение лидера с использованием дашборда Patroni-exporter.
2. Через интерфейс MTS Cloud перезапуск VM, которая в данный момент является лидером.
3. Переход в Grafana для анализа ситуации.

**Ожидаемые результаты:**
Ожидается, что при выходе из строя или недоступности мастера, реплика возьмет на себя ее роль, и это не сильно скажется на доступности системы за счет низкого времени реагирования.

**Реальные результаты:**
В реальности etcd определил недоступность мастера и переключил его в течение 0.15 секунд.

---
![Patroni switch](img/patroni.png)
---
![Patroni log](img/patroni-log.png)

**Анализ результатов:**
Результаты тестирования показали отсутствие некорректной работы кластера PostgreSQL и etcd.

### 2. Имитация частичной потери сети
**План:**
- Заблокировать порт 5432 для tcp между узлами patroni
- Проанализировать поведения системы

**Описание эксперимента:**
1. Фиксация состояния кластера через dashbord-patroni в  Grafana 
2. Заблокировсвать порт 5432 на вм 
```shell
iptables -t nat -A PREROUTING -p tcp -j REDIRECT --to-ports 5432
```
3. Проверки dasbord  что система перестала видет реплику и мастер
4. Разблокировать порт
```shell
iptables -t nat -D PREROUTING -p tcp -j REDIRECT --to-ports 5432
```
5. Анализ поведения системы

**Ожидаемые результаты:**
Ожидается,  что при потери соединения с узлами etcd  будут проводит выборы мастера, как только порты снова станут открытами etcd назначит master и кластер будет работать в штатном режиме 

**Реальные результаты:** 

---
Отключение портов:
![Tcp-off](img/tcp-off.png)

Включение портов:
![Tcp-on](img/tcp-on.png)

---

**Анализ результатов:**
Опираясь на графики было установлено, что система в  течении 10 секунд приходит к рабучему состоянию после включения портов и после задержки от 0.15 - 1с  начинает репликацию данных

### 3.  Высокая нагрузка на CPU

**План:**
- Запустить процессы, которые создают высокую нагрузку на CPU
- Проанализировать поведения системы

**Описание эксперимента:**
1. Определение лидера с использованием дашборда Patroni-exporter.
2. Нагрузка вм при помощт утилиты Chaosblade
```shell
blade create cpu load --cpu-percent 95 --timeout 600
```
3. Переход в Grafana для анализа ситуации.

**Ожидаемые результаты:**
Ожидается, что при высокой нагрузке сменится и система будет работать в штатном режиме

**Реальные результаты:**
В реальности лидер не сменился, из-за чего усугубилось время ответа на запросы, однако сервис работал в штатном режиме 

---
![Responce](img/response.png)
---
![Patroni info](img/patroni-info.png)

**Анализ результатов:**
Думая над этой ситуацией , я пришел к тому , что желательно в таких ситуациях переносить нагрузку на реплику для чтения  запросов или же полного смены мастера, если это не кретично для пользователя,так как при получении ещё большей нагрузки вм может упасть, в результате чего произайдет потеря данных, также в такие периоды достаточно трудно держать приемлемое отстование в данных между мастером и репликой.



### 4.  Тестирование систем мониторинга и оповещения: 

**План:**
- Запустить процессы, которые создают высокую нагрузку на CPU на Patroni
- Дать небольшую нагрузку на систему алертирования для проверки её работы в псевдо  боевой ситуации
- Перзагрузить ВМ
- Проанализировать поведения системы

**Описание эксперимента:**
1. Проверить коректность работы настроенных алертов 
2. Нагрузка вм при помощт утилиты Chaosblade
```shell
blade create cpu load --cpu-percent 60 --timeout 600
blade create mem load --mode ram --mem-percent 65 --timeout 600
```
3. Анализ алертов в Alertmanager

**Ожидаемые результаты:**
Ожидается, что при высокой нагрузке и недоступности вм отправится alert для уведомления о проблеме

**Реальные результаты:**
Алерты отработали корректно и при недоступности в 1 m сообщения об аврии были доставлены 

---
![Alert](img/alert.png)
---

**Анализ результатов:**
Система Alertmanager при заданых условиях сработалла корректно , в результате чего ожидаеммые и реальные тесты сошлись , однако необходимо также учитывать ,что  системаможет повести себя некорректно при высокой нагрузке того хоста , на котором она установлена 

### 5.  Split brain: 

в процессе

### 6.  Долгосрочная изоляция:  

**План:**
- Отключения реплики на длителный период
- Востановление реплики 
- Анализ поведения системы по востановлению 

**Описание эксперимента:**
1. Зафиксировать , что происходит репликация данных 
2. Отключение вм  с репликой на 30 мин - 1 час при помощи интерфейса MTS Cloud
3. Обратное включение узла в кластер
4 Анализ результатов

**Ожидаемые результаты:**
Ожидается, что после включения реплики patroni запустит систему по репликации данных и вернет бд к констестеному состояию.

**Реальные результаты:**
После отключенпи и включения реплики венрнулась в систему и начала получать данный от мастера,lag который возник при включении оказался не существенным,так как основная доля запросов которя летит в api - get запросы. 

---
 Информаци в момент до и после отключения реплики: 

![Replica-off](img/replica-off.png)

![Replica-off-monitoring](img/replica-off-monitoring.png)
---
Информаци после включения реплики:

![Replica-on](img/replica-on.png)

![Replica-on-monitoring](img/replica-on-monitoring.png)

**Анализ результатов:**
На основе полученных данных сделан вывод, что система повела себя корректно и невызвыла  ухудшений в работе при отказе реплик. 


### 7.  Сбои сервисов зависимостей:  

**План:**
Изучить поведение кластера Patroni при сбоях в сопутствующих сервисах, например, etcd (которые используются для хранения состояния кластера),
путем имитации его недоступности или некорректной работы. 

**Описание эксперимента:**
1. Зафикстрофать номальное состояние системы
2. Выводи из строя двух инстансов etc
3. Анализ полученных резульбтатов


**Ожидаемые результаты:**
Ожидается, что после включения etcd не получется коректного смены mater  и replica

**Реальные результаты:**
Получили ситуацию split-brain и оба инстанса стали read only, что привело к некорректному ответу на post запросы.

---
 Информаци в момент отключения etcd: 

![Replica-info](img/replica-info.png)

![Etcd-off](img/etcd-off.png)
---

**Анализ результатов:**
Полученные результаты стали для меня сюрпризом, так как до этого я не сталкивался с работой etcd и ожидал , что даже 1 инстанс etcd  сможет определить master и не допустит split-brain между двумя ингстансами patroni.Анализаруя данный эксперимент, я пришел к выводу, что для системы будет критичен выход из строя 2х etcd , в результате чего она начнет некорректно отрабатывать запросы на запись.
